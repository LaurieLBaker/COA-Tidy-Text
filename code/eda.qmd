---
title: "Text mining with tidy data principles: exploratory data analysis"
author: "Julia Silge"
output: html_document
---

```{r}
#| include: false
my_mirror <- "http://mirrors.xmission.com/gutenberg/"
```

## Access the full text of one book

What book do *you* want to analyze today?

Replace `1342` below with your own choice:
https://www.gutenberg.org/browse/scores/top

```{r}
library(tidyverse)
library(tidytext)
library(gutenbergr)

full_text <- gutenberg_download(1342)
```

Now it's time to tokenize and tidy this text data.

```{r}
tidy_book <- full_text %>%
  mutate(line = row_number()) %>%
  _____

tidy_book
```

What do you predict will happen if we run the following code?

**PREDICT WITH YOUR NEIGHBOR BEFORE YOU RUN**

```{r}
tidy_book %>%
  count(___)
```

## Stop words


```{r}
get_stopwords()
```

Try out some

- different languages (`language`)
- different sources (`source`)

## What are the most common words?

**U N S C R A M B L E**

```{r}
anti_join(get_stopwords(source = "smart")) %>%

tidy_book %>%

count(word, sort = TRUE) %>%

geom_col()

slice_max(n, n = 20) %>%

ggplot(aes(n, fct_reorder(word, n))) +  
```

## Freeland Bunker


library(readxl)
journal_1871_1872 <- read_excel("data/journal_year_1871_1872.xlsx")

journal_1871_1872 %>%
    select(date_mdy, month, journal_entry) %>%
    head()
```
## All Journals

```{r}

journal_1873 <- read_excel("data/journal_1873.xlsx")
journal_1874 <- read_excel("data/journal_year_1874.xlsx")
journal_1875 <- read_excel("data/journal_1875.xlsx")
```

# We want to keep track of the journals

```{r}
journal_1871_1872$journal <- 1
journal_1873$journal <- 2
journal_1874$journal <- 3
journal_1875$journal <- 4


journals <- rbind(journal_1871_1872, journal_1873, journal_1874, journal_1875)

```


## Tidy Journals

```{r}

(tidy_journal <- journals %>%
    select(date_mdy, month, journal_entry, journal) %>%
    unnest_tokens(word, journal_entry))
```

## Most common words

```{r}
tidy_journal %>%
    count(word, sort = TRUE)
```

## Removing stop words


```{r}
#| eval: false
tidy_journal %>%
    anti_join(get_stopwords(source = "smart")) %>%
    count(word, sort = TRUE) %>%
    slice_max(n, n = 20) %>%
    ggplot(aes(n, fct_reorder(word, n))) +  
    geom_col()
```

## Journal 1: Boats, Meals, Goods ðŸ³ â›µðŸ¦ž ðŸªµ


```{r}
#| echo: false
tidy_journal %>%
    anti_join(get_stopwords(source = "smart")) %>%
    filter(journal == 1) %>%
    count(word, sort = TRUE) %>%
    slice_max(n, n = 25) %>%
    mutate(color = case_when(word %in% c("breakfast", "dinner", "supper", "tea") ~ "meal", TRUE ~ "other")) %>%
    ggplot(aes(n, fct_reorder(word, n))) +  
    geom_col(aes(fill = color)) +
    labs(fill = "Word Type", y = "word")
```

## Journal 2: Wind and Weather â˜ï¸Ž NESW

```{r}

tidy_journal %>%
    anti_join(get_stopwords(source = "smart")) %>%
    filter(journal == 2) %>%
    count(word, sort = TRUE) %>%
    slice_max(n, n = 25) %>%
    mutate(color = case_when(word %in% c("wind", "n.w", "north", "east", "west", "south", "rainy", "heavy", "snow", "breeze", "easterly", "southerly", "moderate") ~ "wind and weather", TRUE ~ "other")) %>%
    ggplot(aes(n, fct_reorder(word, n))) +  
    geom_col(aes(fill = color)) +
    labs(fill = "Word Type", y = "word")
```

## Journal 3: What themes do you see for journal 3?

```{r}
#| echo: false
tidy_journal %>%
    anti_join(get_stopwords(source = "smart")) %>%
    filter(journal _______) %>%
    count(word, sort = TRUE) %>%
    slice_max(n, n = 25) %>%
    #mutate(color = case_when(word %in% c(_______) ~ "______", TRUE ~ "other")) %>%
    ggplot(aes(n, fct_reorder(word, n))) +  
#    geom_col(aes(fill = color)) +
#    labs(fill = "Word Type", y = "word")
```


## Sentiment lexicons

Explore some sentiment lexicons.

```{r}
get_sentiments("bing")
```

Implement sentiment analysis with an `inner_join()`

```{r}
tidy_book %>%
  ___(get_sentiments("bing")) %>%
  count(sentiment, sort = TRUE)
```


What do you predict will happen if we run the following code?

**PREDICT WITH YOUR NEIGHBOR BEFORE YOU RUN**

```{r}
tidy_book %>%
  inner_join(get_sentiments("bing")) %>%            
  ___
```

What words contribute the most to sentiment scores for **your** book?

```{r}
tidy_book %>%
  inner_join(get_sentiments("bing")) %>%
  count(sentiment, word, sort = TRUE) %>%
  group_by(sentiment) %>%
  slice_max(n, n = 10) %>%
  ungroup %>%
  ggplot(aes(n,
             ___, 
             fill = sentiment)) +
  geom_col() +
  facet_wrap(~ sentiment, scales = "free") 
```


## Term frequency and inverse document frequency

Go back to Project Gutenberg and make a collection (*corpus*) for yourself!

```{r}
full_collection <- gutenberg_download(c(141, 158, 161, 1342),
                                      meta_fields = "title",
                                      mirror = my_mirror)

full_collection
```

Count word frequencies in your collection.

```{r}
book_words <- full_collection %>%
  ___
  count(title, word, sort = TRUE)

book_words
```

Calculate tf-idf.

```{r}
book_tf_idf <- book_words %>%
  ___

book_tf_idf
```

What do you predict will happen if we run the following code?

**PREDICT WITH YOUR NEIGHBOR BEFORE YOU RUN**


```{r}
book_tf_idf %>%
  ___
```

**U N S C R A M B L E**

```{r}
group_by(title) %>%

book_tf_idf %>%

slice_max(tf_idf, n = 10) %>%

ggplot(aes(tf_idf, fct_reorder(word, tf_idf), fill = title)) +

facet_wrap(~title, scales = "free")

geom_col(show.legend = FALSE) +
```

Weighted log odds is another approach for measuring what a document is about.

```{r}
library(tidylo)

book_words %>%
    ___(title, word, n) %>%
    arrange(-log_odds_weighted)
```

## N-grams... and BEYOND

```{r}
tidy_ngram <- full_text %>%
  unnest_tokens(___)

tidy_ngram
```

What are the most common bigrams?

```{r}
tidy_ngram %>%
  ___
```

Let's use `separate()` from tidyr to remove stop words.

```{r}
bigram_counts <- tidy_ngram %>%
  ___
  filter(!word1 %in% stop_words$word,
         !word2 %in% stop_words$word) %>%
  count(word1, word2, sort = TRUE)

bigram_counts
```

## Network analysis

Create a word network from bigrams!

```{r}
library(widyr)
library(ggraph)
library(tidygraph)

bigram_graph <- bigram_counts %>%
  filter(n > 5) %>%
  ___

bigram_graph
```

Visualize the network.

```{r}
bigram_graph %>%
  ggraph(layout = "kk") +
  geom_edge_link(___) + 
  geom_node_text(___) +    
  theme_graph() 
```


Lots of ways to make the graph nicer!

```{r}
bigram_graph %>%
  ggraph(layout = "kk") +
  geom_edge_link(___,
                 show.legend = FALSE, 
                 arrow = arrow(length = unit(1.5, 'mm')), 
                 start_cap = circle(3, 'mm'),
                 end_cap = circle(3, 'mm')) + 
  geom_node_text(___) +    
  theme_graph() 
```

Thanks for joining! <3
